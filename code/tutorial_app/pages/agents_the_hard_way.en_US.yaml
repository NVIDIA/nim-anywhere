# header data
title: Agents the Hard Way
waiting_msg: Let me know when you are ready.
testing_msg: Waiting on you to finish up.
next: Let's go!

welcome_msg: |
  In this excercise, we will be building a simple AI agent from scratch.
  Later, we will replace much of this logic with prebuilt tools, but building it ourselves helps
  us understand the fundamentals.

  ![intro picture](app/static/agent-lab.png)

tasks:
  - name: What is an agent?
    msg: |
      An agent is an application that uses an LLM to make decisions and interact with the outside world.

      Agents have four key parts:
      1. **MODEL:** An LLM that decides which tools to use and how to respond
      1. **TOOLS:** Functions that let the LLM perform actions like math or database queries
      1. **MEMORY:** Information available to the LLM during and between conversations
      1. **LOGIC:** The decision-making flow that determines which, if any, tools to use next

      In this exercise, we'll build a basic agent by implementing these basic components from scratch.

  - name: Setup the environment
    msg: |
      To get us started, I'll drop is some code and explain as I go.

      Like most Python code, we start by importing the necessary modules.
      - The `openai` module is maintained by OpenAI and is used for talking to models running remotely.
      - The `caching` module helps us cache LLM responses and conserve tokens.

      The caching module is custom code, but it makes use of the `cachier` Python library.
      Feel free to check it out by opening Visual Studio Code or JupyterLab from the Workbench interface.
    response: |
      Let's keep going!
    prep: prep_imports

  - name: Load the configuration
    msg: |
      Now I'll load our configuration as contants:
      - `API_KEY` loads our credentials from an environment variable
      - `MODEL_URL` points the the server hosting your model
      - `MODEL_NAME` is the model we are going to use

      Why these values?

      This `MODEL_URL` points to NVIDIA's hosted Model API Catalog.

      Becuase we are starting with NVIDIA's hosted service, we have a lot of models to choose from.
      Picking where to start can be difficult.

      - Start with a newer open source model from a team you recognize.
      - Start with a moderate sized model (~70b parameters). You'll work on optimizing to a smaller model later.
      - If you need features like function calling, make sure the model supports it! (More on that later).
    response: |
      Let's keep going!
    prep: prep_api_key

  - name: Part 1 - The Model
    msg: |
      The first of four critical parts for an agent is the AI model.

      We will talk to the AI models on build.nvidia.com using the OpenAI API.
      This API is the *language* that most model providers use.
      This means we can use the `OpenAI` class to connect to most model providers. Neat!

      Using the `MODEL_URL` and `API_KEY` defined above, create a new model client named `client`.

      #### Resources
      - [Docs: OpenAI Quick Start](https://github.com/openai/openai-python?tab=readme-ov-file#usage)
      - [Docs: OpenAI Custom base URL](https://github.com/openai/openai-python?tab=readme-ov-file#configuring-the-http-client)
      - [Example: NVIDIA hosted models](https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1)

      <details>
        <summary>Need some help?</summary>

        ```python
        client = OpenAI(
            base_url=MODEL_URL,
            api_key=API_KEY
        )
        ```
      </details>
    response: |
      Great! We are now connected to the models!
    prep: prep_define_client
    test: test_define_client

  - name: Part 2 - Tools
    msg: |
      Every agent has access to some tools.
      Tools are how the model is able to interact with the world.

      Tools are created by developers to connect to external services.
      The model cannot *do* anything that it doesn't have a tool for.

      Tools are simply code that is executed at the LLM's request.
      So lets write our first tool!

      Create a function, called `add`, that adds two integers, called `a` and `b`.

      <details>
        <summary>Need some help?</summary>

        Here is a simple Python function for adding two numbers.

        ```python
        def add(a, b):
          return a + b
        ```

        We can use this function as a tool by returning it from our function.
      </details>
    response: ~
    prep: prep_adding_tool
    test: test_adding_tool

  - name: Create a menu of tools
    msg: |
      #asdf
    response: |
      Next, we create a list of tools and their descriptions. This metadata is
      given to the LLM so it knows what tools it has available and how to use them.

      We will create a variabled named `tools` that is a list of dictionaries.
      The dictionaries have a specific structure, defined by OpenAI, to define
      a function. Create a single function definition in `tools` that describes
      your `add` fucntion, its arguments, and short descriptions.

      #### Resources
      - [Docs: Defining functions format](https://platform.openai.com/docs/guides/function-calling?api-mode=responses&example=get-weather#defining-functions)

      <details>
        <summary>Need some help?</summary>

        Here is an example of a minimal tools definition for out `add` function.

         ```python
         tools = [
             {
                 "type": "function",
                 "function": {
                     "name": "add",
                     "description": "Add two integers.",
                     "parameters": {
                         "type": "object",
                         "properties": {
                             "a": {"type": "integer", "description": "First integer"},
                             "b": {"type": "integer", "description": "Second integer"},
                         },
                         "required": ["a", "b"],
                     },
                 },
             }
         ]
         ```
      </details>
    test: define_tools_list


  - name: Create short term memeory
    msg: |
      Our agent will need to be able to remember a few things. To keep everything simple, we can use a list for our memory.
      Create a variable named `messages` that is a list. Our list will have a single entry that will be a dictionary
      representing a message from the user asking, "What is 3 plus 12?". The user message, in dictionary format
      is: `{"role": "user", "content": "What is 3 plus 12?"}`.

      Every message from the model, the user, or a tool, is added to this list for the model to use later.

      #### Reference
      - [Docs: Chat completion docs](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)

      <details>
        <summary>Need some help?</summary>

        Here is an example of a minimal messages definition.

        ```python
        messages = [
          {"role": "user", "content": "What is 3 plus 12?"}
        ]
        ```

      </details>
    response: ~
    test: define_messages

  - name: Ask the model how to proceed
    msg: |
      Next, we will use the chat completions API to ask the model what to do next.

      Use the `client` to create a chat completions request. Pass the `messages` and `tools` variables.
      Save the response to a variable named `response`.

      <details>
        <summary>Need some help?</summary>

        Here is an example of a minimal chat completion request.

        ```python
        response = client.chat.completions.create(
            model="meta/llama-3.3-70b-instruct",
            messages=messages,
            tools=tools,
        )
        ```
      </details>

  - name: Route tool calls
    msg: |
      Now we inspect the response from the model. The response contains a tool call, indicating
      that the model wants us to execute a tool and return the results.

      Write some quick logic

      <details>
        <summary>Need some help?</summary>

        Here is a tool router logic.

        ```python
        if "add" == tool_name:
            # Get the function from the global namespace
            result = add(**tool_args)
            print(f"Result of {tool_name}({tool_args}): {result}")
        ```
      </details>
    response: ~
    test: execute_tool

  - name: Create a new message list with the tool call + response
    msg: |
      #asdf
    response: ~
    test: create_message_list

  - name: Call again to let the model continue the chat
    msg: |
      #aaa
    response: ~
    test: call_model_again

info_test_nonzero_exit_code: "Uh oh! Looks like your code isn't running. Check the *Terminal Output* tab."
info_no_client: "Waiting for the `client` variable."
info_wrong_client_type: "`client` needs to be an instance of the OpenAI class."
info_client_bad_request: |
  We got a Bad Request error. This usually happens if you give the wrong args to the `chat.completions.create` method.
info_client_bad_auth: |
  You API Key was rejected.
  Check the value you set in the workbench interface in the Project Container --> Variables configuration.
info_test_bad_url: "No models were found at the provided URL. Make sure we are pointing to a working endpoint."
info_no_add: "Waiting for the `add` function."
info_add_not_fun: "`add` needs to be a funtion."
info_bad_add_args: "Check the arguments to your `add` function."
info_add_not_working: "The `add` function doesn't seem to be adding."
