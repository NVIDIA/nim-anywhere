# header data
title: Agents the Hard Way
waiting_msg: Let me know when you are ready.
testing_msg: Waiting on you to finish up.
next: Let's go!

welcome_msg: |
  In this excercise, we will be building a simple AI agent from scratch.
  Later, we will replace much of this logic with prebuilt tools, but building it ourselves helps
  us understand the fundamentals.

  ![intro picture](app/static/agent-lab.png)

tasks:
  - name: What is an agent?
    msg: |
      An agent is an application that uses an LLM to make decisions and interact with the outside world.

      Agents have four key parts:
      1. **MODEL:** An LLM that decides which tools to use and how to respond
      1. **TOOLS:** Functions that let the LLM perform actions like math or database queries
      1. **MEMORY:** Information available to the LLM during and between conversations
      1. **LOGIC:** The decision-making flow that determines which, if any, tools to use next

      In this exercise, we'll build a basic agent by implementing these basic components from scratch.

  - name: Setup the environment
    msg: |
      To get us started, I'll drop is some code and explain as I go.

      Like most Python code, we start by importing the necessary modules.
      - The `openai` module is maintained by OpenAI and is used for talking to models running remotely.
      - The `caching` module helps us cache LLM responses and conserve tokens.

      The caching module is custom code, but it makes use of the `cachier` Python library.
      Feel free to check it out by opening Visual Studio Code or JupyterLab from the Workbench interface.
    response: |
      Let's keep going!
    prep: prep_imports

  - name: Load the configuration
    msg: |
      Now I'll load our configuration as contants:
      - `API_KEY` loads our credentials from an environment variable
      - `MODEL_URL` points the the server hosting your model
      - `MODEL_NAME` is the model we are going to use

      Why these values?

      This `MODEL_URL` points to NVIDIA's hosted Model API Catalog.

      Becuase we are starting with NVIDIA's hosted service, we have a lot of models to choose from.
      Picking where to start can be difficult.

      - Start with a newer open source model from a team you recognize.
      - Start with a moderate sized model (~70b parameters). You'll work on optimizing to a smaller model later.
      - If you need features like function calling, make sure the model supports it! (More on that later).
    response: |
      Let's keep going!
    prep: prep_api_key

  - name: Part 1 - The Model
    msg: |
      The first of four critical parts for an agent is the AI model.

      We will talk to the AI models on build.nvidia.com using the OpenAI API.
      This API is the *language* that most model providers use.
      This means we can use the `OpenAI` class to connect to most model providers. Neat!

      Using the `MODEL_URL` and `API_KEY` defined above, create a new model client named `client`.

      #### Resources
      - [Docs: OpenAI Quick Start](https://github.com/openai/openai-python?tab=readme-ov-file#usage)
      - [Docs: OpenAI Custom base URL](https://github.com/openai/openai-python?tab=readme-ov-file#configuring-the-http-client)
      - [Example: NVIDIA hosted models](https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1)

      <details>
        <summary>Need some help?</summary>

        ```python
        client = OpenAI(
            base_url=MODEL_URL,
            api_key=API_KEY
        )
        ```
      </details>
    response: |
      Great! We are now connected to the models!
    prep: prep_define_client
    test: test_define_client

  - name: Part 2 - Tools
    msg: |
      Every agent has access to some tools.
      Tools are how the model is able to interact with the world.

      Tools are created by developers to connect to external services.
      The model cannot *do* anything that it doesn't have a tool for.

      Tools are simply code that is executed at the LLM's request.
      So lets write our first tool!

      Create a function, called `add`, that adds two integers, called `a` and `b`.

      <details>
        <summary>Need some help?</summary>

        Here is a simple Python function for adding two numbers.

        ```python
        def add(a, b):
          return a + b
        ```

        We can use this function as a tool by returning it from our function.
      </details>
    response: ~
    prep: prep_adding_tool
    test: test_adding_tool

  - name: Describe the tools
    msg: |
      Before moving on, we need create a description of the tools that the model can understand.
      Think of this as the LLM's menu of possible helpers.

      The syntax on this is tricky and picky, so I'll do this part.
      The good news is this is usualy handled for you by agent frameworks.
      This syntax is defined as part of the OpenAI API spec.

      #### Resources
      - [Docs: Defining functions format](https://platform.openai.com/docs/guides/function-calling?api-mode=responses&example=get-weather#defining-functions)
    response: Your turn again!
    prep: prep_tools_list


  - name: Part 3 - Memory
    msg: |
      The topic of memory is complex, and we will only scratch the surface.
      There are two types of memory, short term and long term.
      For now, lets focus on short term memory.

      Short term memory starts at the beginining of the conversation
      and ends at the end of the conversation.
      Put simply, short term memory is a log of the conversation.

      For this, we will use a humble list.
      Every line in our list will be a message in the conversation, stored in a dictionary.
      The messages can come from the user, the assistant, or from tools.

      Create an initial list called `memory`.
      Initialize it with this message from the user:

      ```python
      {"role": "user", "content": "What is 3 plus 12?"}
      ```

      #### Reference
      - [Docs: Chat completion docs](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)

      <details>
        <summary>Need some help?</summary>

        ```python
        messages = [
          {"role": "user", "content": "What is 3 plus 12?"}
        ]
        ```

      </details>
    response: Looking good. Now let's have some fun!
    prep: prep_messages
    test: test_messages

  - name: Ask the model how to proceed
    msg: |
      Next, we will use the chat completions API to ask the model what to do next.

      Use the `client` to create a chat completions request. Pass the `messages` and `tools` variables.
      Save the response to a variable named `response`.

      <details>
        <summary>Need some help?</summary>

        Here is an example of a minimal chat completion request.

        ```python
        response = client.chat.completions.create(
            model="meta/llama-3.3-70b-instruct",
            messages=messages,
            tools=tools,
        )
        ```
      </details>

  - name: Route tool calls
    msg: |
      Now we inspect the response from the model. The response contains a tool call, indicating
      that the model wants us to execute a tool and return the results.

      Write some quick logic

      <details>
        <summary>Need some help?</summary>

        Here is a tool router logic.

        ```python
        if "add" == tool_name:
            # Get the function from the global namespace
            result = add(**tool_args)
            print(f"Result of {tool_name}({tool_args}): {result}")
        ```
      </details>
    response: ~
    test: execute_tool

  - name: Create a new message list with the tool call + response
    msg: |
      #asdf
    response: ~
    test: create_message_list

  - name: Call again to let the model continue the chat
    msg: |
      #aaa
    response: ~
    test: call_model_again

info_test_nonzero_exit_code: "Uh oh! Looks like your code isn't running. Check the *Terminal Output* tab."
info_no_client: "Waiting for the `client` variable."
info_wrong_client_type: "`client` needs to be an instance of the OpenAI class."
info_client_bad_request: |
  We got a Bad Request error. This usually happens if you give the wrong args to the `chat.completions.create` method.
info_client_bad_auth: |
  You API Key was rejected.
  Check the value you set in the workbench interface in the Project Container --> Variables configuration.
info_test_bad_url: "No models were found at the provided URL. Make sure we are pointing to a working endpoint."
info_no_add: "Waiting for the `add` function."
info_add_not_fun: "`add` needs to be a funtion."
info_bad_add_args: "Check the arguments to your `add` function."
info_add_not_working: "The `add` function doesn't seem to be adding."
info_no_messages: "Waiting for the `messages` variable."
info_messages_not_correct: "Check the contents of your `messages` variable."
